{\rtf1\ansi\ansicpg1252\deff0\nouicompat{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset2 Symbol;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Riched20 10.0.10586}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs28\lang9 Neural Network and Autoencoder Project Log\line\fs22 Devin Denis\par
This document is for me to record my thoughts as I work on implementing a simple neural network, autoencoder, and backpropagation algorithm.\par
\b 5/02/2016\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Initialization and update were quite straightforward.  \fs28\par
{\pntext\f1\'B7\tab}\fs22 I decided to use global variables for the nonlinear function and derivative so I can easily swap them out.  I probably wouldn't have made them global, but they don't fit very well into the parameter structure set up in the template.  I'm starting with tanh, as the material I read was using that, but I'll probably also try sigmoid.  I've also heard soft-max is a good choice for the last layer of a multi-class ANN, might look into that later.\fs28\par
{\pntext\f1\'B7\tab}\fs22 I have all the mnist stuff in the same base folder as my project code right now, should really fix that.\fs28\par

\pard\sa200\sl276\slmult1\b\fs22 6/02/2016\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Up till now I had been thinking about the threshold nodes as just being an extra neuron in each layer.  But they actually work a bit differently, since they don't get input.  So I decided to change up my original design a bit.  The state and connection variables will now be lists with the original state and connection plus the threshold info in them.  Probably would've separated those normally, but we'll stick to the template.\par
{\pntext\f1\'B7\tab}You know, sometimes I forget python is an object oriented language.  I could make my list variables into classes.  I think I'll leave it for now, since they're still very simple, but if they get more complicated in the future, that's probably a good idea.\par
{\pntext\f1\'B7\tab}I think I've got backpropagation done.  Needs testing.  Guess we'll see how it goes.\par
{\pntext\f1\'B7\tab}Currently working through getting backpropagation working.  Mostly going well.  However, I realized I hadn't normalized the input.  A bit of research indicated that doing so is desirable, so I think I'll go ahead and do that.\par
{\pntext\f1\'B7\tab}Confused myself a bit over weight indicies.  Seems like in the material I was following, W[i] is the weights from layer i-1 to layer i.  In my code W[i] is the ith set of weights, which goes from layer i to layer i+1.  If that wasn't the case, W[0] wouldn't make much sense.  So yeah, lots of off-by-one errors.\par
{\pntext\f1\'B7\tab}Still having some issues with indicies lining up properly.  Making progress though.\par

\pard\sa200\sl276\slmult1\b 9/02/2016\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Got all the indicies lining up properly.  Everything ends up the right shape.  Not actually getting any good results yet, though.\par
{\pntext\f1\'B7\tab}Going to start with a few easy changes to improve the training process- if that doesn't improve things then I've got a bug with backpropagation I'll need to look into.\par
{\pntext\f1\'B7\tab}So currently the output neurons produce all 1 or all -1.  Pretty likely that's due to some backpropagation error.  I have one other thing I'm going to try before I dive into that, though.  So far I've been doing stochastic gradient descent.  I'm going to give regular gradient descent a try.\par
{\pntext\f1\'B7\tab}My global parameters are building up.  I should probably make them into command line arguments at some point.\par

\pard\sa200\sl276\slmult1\b 13/02/2016\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Gonna start today with some advice I've seen online a few times.  I'm going to ignore the MNIST data for the moment and try to train a really basic XOR function.  I'll watch the backpropagation while that goes and try to figure out what's going wrong.\par
{\pntext\f1\'B7\tab}I just realized that my tanh derivative function was wrong.  Apparently when I wrote it I changed 1 - tanh^2 into tanh^2 - 1.  Woops.  That would certainly explain the nonsense results.  \par
{\pntext\f1\'B7\tab}Ah, another issue- I had my nonlinear derivatives defined with respect to the input to a node, not the output from it.  So instead of (1 - tanh(x)^2) of input, I was actually doing (1 - tanh(tanh(x))^2). \par
{\pntext\f1\'B7\tab}Found a step through of backpropagation on a simple network online.  It was quite helpful in checking if my numbers were coming out the same.  {{\field{\*\fldinst{HYPERLINK http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ }}{\fldrslt{http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\ul0\cf0}}}}\f0\fs22\par
{\pntext\f1\'B7\tab}So I can now get the network to converge to the correct value for XOR with a network size of 2,2,2, gradient descent, learning rate 0.5, and 5000 epochs.  That seems like a lot of work to get it to recognize a pretty simple function.  Still, I'm now reasonably confident my backpropagation implementation is actually correct.  Going to try going back to the MNIST data now and see how it goes.\par
{\pntext\f1\'B7\tab}Sigmoid of 0 is 0.5.  Right.  That does not work with the plan to just zero out the unused neurons.  Maybe I should just make a list of layers, instead of having them in an ndarray.  It's not like I'm vectorizing anything over the layer axis, each of those has to go in one part.  And I suppose I'm doing extra calculations for all the zeros.\par
{\pntext\f1\'B7\tab}Just switching to tanh (which is 0 at 0) resulted in a 16% successful classification rate without playing with the parameters at all.  That's not exactly a success, but it's better than picking randomly!\par
{\pntext\f1\'B7\tab}Happened across a site describing some results that showed the interval in which weights should be randomly initialized.  It's easy to add, so I'm going to go ahead and do so. {{\field{\*\fldinst{HYPERLINK http://deeplearning.net/tutorial/mlp.html }}{\fldrslt{http://deeplearning.net/tutorial/mlp.html\ul0\cf0}}}}\f0\fs22\par
{\pntext\f1\'B7\tab}Looking at the MNIST page of existing results, the 2-3 layer neural networks have 300+ hidden units.  I have 28 (I decided to take the square root of the number of input neurons).  Going to try increasing the number.\par
{\pntext\f1\'B7\tab}90% Successful prediction!  What a jump from the last run (18%)!  I set my number of hidden units up to 300, set the learning rate down to 0.1, and increased the number of training runs.  Only problem is, now I'm not sure which of those were the most important.  On the one hand, I want to do lots of tests to check which parameters will work the best.  On the other, each test takes a not-insignificant amount of time.\par
{\pntext\f1\'B7\tab}Ah, but I'm getting ahead of myself.  That prediction is on the training set, I should check it on the test set:\line 86%, still pretty good. \par
{\pntext\f1\'B7\tab}I know there's still plenty of improvement I can get out of this setup, but I think I should move on to implementing the autoencoder and autoencoder classifier, now that I've got a decent result.  Once I've got those working I can go back to tweaking things.\par

\pard\sa200\sl276\slmult1\b 15/02/2016\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Before I start with the autoencoder, I have to make a couple of changes to my current code.  The way I handle labels right now is to just pass in the index in the output which we want to be 1.  This won't work for the autoencoder.  Instead, we'll want to pass in a vector.  I think we might as well make this change for the regular network as well, and just add a helper function to get the appropriate vector from an index.\par
{\pntext\f1\'B7\tab}I also want to make the change I discussed previously regarding removing the zero entries and switching to a list of layers rather than 3D ndarray.  I actually intend to stick with tanh, but I think it may be worth it from an efficiency perspective (computing less 0 * x steps). \par
{\pntext\f1\'B7\tab}Realized that since the range of my tanh is -1 to 1, I should have my label vectors -1 everywhere but the correct index, rather than 0 everywhere but the correct index.  Though for sigmoid that would be wrong I think.  Maybe I'll make it definable in the nonlinear-switching section.  Switching it got a small boost in accuracy (88% correct on the test set now).\par
}
 